\documentclass[main.tex]{subfiles}
\begin{document}
\section{Introduction}

\subsection{Dead Reckoning}\label{sec:deadReckon}
% explain the issues of dead Reckoning
% use as an introduction: 
% 	to the FWKin prob.
% 	to the different sensors used

%todo: fix 1st sent.

Robot localization is crucial to effective robot locomotion. Various methods are
used to determine the current and future locations of robot; each unique to the
available sensors, robot structure and environment. Dead reckoning is one of the
simpler methods used. It requires as few sensors as possible and relies mostly
on known motion models and states.  Dead reckoning is a method of finding the
location of an object based on a known motion model and current state
information. Once the current state of an object is known then its motion model
can be used to predict future motion states. For example, one can predict the
future position of a robot if it is known that that the robot is moving in a
straight line and at a constant speed and its current position.

A two wheeled robot as shown in \figref{2wheelBot} will be used for our analysis
of dead reckoning. Two wheeled odometry is used to create a motion model of the
robot which is then used to predict future locations of the robot. In this study
only the $x$ and $y$ coordinates are of interest although velocity can easily
derived. 

\begin{figure}[H]
\begin{center}
\include{2wheelBot}
\end{center}
\caption{2 Wheeled Robot (Side view)}
\label{fig:2wheelBot}
\end{figure}

We will begin our analysis with the odometry model of the two wheeled robot. The
odometry model is as follows (its derivation is beyond the scope of this
report):

\begin{equation}
\label{eq:deadReckonState}
	P(t) = 
	\begin{bmatrix}
	S \\ \theta
	\end{bmatrix}
	=
	\begin{bmatrix}
	\frac{1}{2}	& \frac{1}{2} \\[0.3em]
	\frac{1}{\mathrm{w}} & -\frac{1}{\mathrm{w}}
	\end{bmatrix} 
	\begin{bmatrix}
	S_R(t) \\ S_L(t)
	\end{bmatrix}
\end{equation}

Where:
\begin{itemize}
\item $S$ is the robot distance travelled.
\item $\theta$ is the heading of the robot.
\item $S_L$ and $S_R$ are the distances traveled by each wheel (left and right
respectively).
\item $w$ is the distance between the contact points of the
wheels of the robot.
\end{itemize}

\begin{figure}[H]
	\begin{center}
	\input{2wheelMotion}
	\end{center}
	\caption{Robot Motion Model from $t=0 -$ \delt}
	\label{fig:2wrMotion}
\end{figure}


From \eqref{deadReckonState} we can create a motion model relative to the
inertial frame of reference $I$ (as shown in \figref{2wrMotion}). To simplify
our analysis the following is assumed:

\begin{itemize}
\item The robot's motion model is measured on a time differential \delt that
is small with respect to the total time ($T$) the robot performs its motion. 
\item The robot's wheel encoders are error-less and no wheel slipping occurs.
\item \delt is chosen to be small enough so that $S\mathrm{'}-S \approx 0$ and
thus $S\mathrm{'} = S$.
\end{itemize}

From these assumptions we have a motion model for the robot:

\begin{equation}
\label{eq:motionState}
\xi_{i} = 
\begin{bmatrix}
x \\ y \\ \theta
\end{bmatrix}
=
\begin{bmatrix}
\cos{\theta} & 0\\
\sin{\theta} & 0\\
0 & 1
\end{bmatrix}
P(t_i+\Delta\mathrm{t})-P(t_i)
\end{equation}

$\xi_{i}$ is the position of the robot after some time lapse \delt. Thus to get
the current position of the robot after some time $T$ simply sum the interval
position changes\footnote{A continuous form of \eqref{sumE} is beyond ths scope
of this lab and is not needed computationally.}: 

\begin{equation}
\label{eq:sumE}
\xi(T) = \xi_0 + \sum^{n}_{i=0} \xi_{i} : n = \frac{T}{\Delta\mathrm{t}}
\end{equation}

\eqref{sumE} simply means that given the robots initial position ($\xi_0$) and a sequence
of wheel distance measurements for $t=\{\delT,2\delT,\cdots,T\}$ the position of
the robot at $T$ can be predicted. This of course is only valid for the above
assumptions.

Dead reckoning is not practical when used alone. Sensor noise, wheel slippage,
and distance estimation error ($S-S\mathrm{'}$) all affect the outcome of
\eqref{sumE}. However dead reckoning is not completely useless. In fact it forms
the base motion model for other filter and motion estimation algorithms (such as
the Kalman filter\footnote{\url{https://en.wikipedia.org/wiki/Kalman_filter}}).

\subsection{Wheel Encoders}
% encoding) measuring x/v/a from encoders sources of error

As seen in \eqref{deadReckonState} the robot's motion model is dependent on
wheel distance travelled. Sensors that can translate rotational wheel motion to
linear distance traveled are called wheel encoders. Different types of encoders
are available each with their advantages and disadvantages. In this study we
will examine a quadrature wheel encoder. 

A quadrature encoder uses a disk (as shown in \figref{quadEncode}) and two
optical sensors to track the position of a wheel. The disk is patterned with
alternating opaque and transparent wedges. As the disk spins the optical sensors
produce a logic signal (0 or 1) respective to the wedge type it is over. The two
sensors are placed in such a manner that their outputs are $90\degree$ out of
phase (see \figref{encSig}). The encoder is coupled with the wheel in such a
manner that the disk spins with an angular velocity that is proportional to that
of the wheel.

\begin{figure}[h]
	\begin{center}
	\include{quadEncode}
	\end{center}
	\caption{Quadrature Encoder}
	\label{fig:quadEncode}
\end{figure}

\begin{figure}[h]
	\begin{center}
	\include{encodeSig}
	\end{center}
	\caption{Quadrature Encoder Signals ($90\degree$ phase shift)}
	\label{fig:encSig}
\end{figure}

\begin{table}[h]
	\begin{center}
	\begin{tabularx}{.35\textwidth}{ccc}
		\toprule
		Phase($\rho$) & A Signal & B signal\\
		\midrule
		0 & 0 & 0 \\
		1 & 1 & 0 \\
		2 & 1 & 1 \\
		3 & 0 & 1 \\
		\bottomrule
		\end{tabularx}
		\caption{Phases of Quadrature Encoder}
		\label{tab:phaseTab}
	\end{center}
\end{table}

The output from the encoder can be viewed as a 4 phase signal (see
\tabref{phaseTab}). Each phase change is called a ``pulse'' and is detected by
an observing hardware (typically a MCU). Software can then be used to determine
the rotation state of the wheel by analyzing the encoder phase after every pulse
it produces.

The direction of the encoder can be determined by two successive phase changes.
For example if the current phase is 2 and on the next encoder reading the phase
is now 1 the encoder is rotating clockwise. If determining the encoder phase is
not performed within a certain amount of time then the software will miss some
encoder ticks resulting in erroneous calculations. 

Because the encoder uses wedges to indicate rotation the encoder's resolution is
restricted to a certain number of pulses per rotation angle. This means that
their is a minimum distance the wheel must travel in order for the encoders to
detect motion.

The lateral displacement of a wheel at some time $t$ for each wheel is: 

\begin{equation}
\label{eq:encTransDist}
S_{L,R}(t) = R\mu\sum_{i=1}^n{(-1)^{f(\rho_i,\rho_{i-1})}}
\end{equation}

Where: 
\begin{itemize}
\item $\mu$ is the encoder resolution in $[\frac{radians}{pulse}]$
\item $n$ is the number of pulses from $t=0$ to $t$
\item $f(\rho_i,\rho_{i-1})$ is a function that takes the ``current'' phase and
the previous phase and returns $2$ or $1$ if the direction between the two
phases is forward or reverse respectively.
\end{itemize}

The translational velocity and acceleration of the wheels can be found from \eqref{encTransDist}:

\begin{eqnarray}
\label{eq:encTransSpeed}
\dot{S}_{L,R}(t) = \frac{S_{L,R}(t)-S_{L,R}(t-\delT)}{\delT}
\\
\ddot{S}_{L,R}(t) = \frac{\dot{S}_{L,R}(t)-\dot{S}_{L,R}(t-\delT)}{\delT}
\end{eqnarray}

Note that \delt is the same as defined in \secref{deadReckon}. Also note that
$\dot{S}_{L,R}(-\delT) = 0$ and $\ddot{S}_{L,R}(-\delT) = 0$ because any
movement previous to $t=0$ does not exist and to make $S_{L,R}(t)$ and
$\dot{S}_{L,R}(t)$ be the initial velocity and acceleration.

Wheel encoders are not perfect and are subjected to error. Encoder errors can
occur from many sources including: electric noise, slippages, temperature, and
wheel velocity. As stated above if the wheel being measured is moving faster
than the decoding algorithm ($f(\rho_i,\rho_{i-1})$) can run then encoder
missing will happen causing the robot to possibly become disoriented. Thus
encoders perform best when the robot is being driven on a smooth, flat surface
and their is little to no slippage of the wheels. 

\subsection{Sonar Range Finder}
Wheel encoders are not the only sensors used to determine a robot's position.
Often circumstances require that a robot knows its position relative to objects
in its environment. Wheel encoders cannot provide this form of information.
Instead range finding sensors are used to give the robot information about how
far away objects are from it's current position. One such device is the sonar
range finder. 

\begin{figure}[h]
	\begin{center}
		\include{ultraSonic}
	\end{center}
	\caption{Ultrasonic Range Finder}
	\label{fig:ultraSon}
\end{figure}

A sonar range finder uses ultrasonic sound waves to detect nearby objects. The
finder is made of two main components: an emitter and receiver. The emitter
sends out a high frequency sound wave which is then reflected back to the
sensor. The receiver then detects when the reflected sound wave (see
\figref{ultraSon}) returns to the sensor. The time it takes for the sound wave
to travel from the sensor to the object and back is called the time of flight
($\delT_f$) and is used to calculate the distance from the object. 

\begin{figure}[H]
	\begin{center}
		\include{ultraSig}
	\end{center}
	\caption{Sonar Finder Signals}
	\label{fig:ultraSig}
\end{figure}

\figref{ultraSig} shows a sample pulse and receive sequence by the sonar range
finder. After the signal is emitted the sensor counts the time (shown as the
``Time Lapse Integral'') it takes before it receives an echo that meets a set
threshold. Once $\delT_f$ is known the distance from the object can be found: 

\begin{equation}
\label{ultDEq}
d = \frac{c_{air}\delT_f}{2}
\end{equation}

Where: 
\begin{itemize}
\item $c_{air}$ is the speed of sound in air
\item $\delT_f$ is the time of flight
\end{itemize}

\figref{ultraSon} is slightly misleading. It implies that a single pulse from the
sensor can be used to determine the distance towards an object. In reality the
sound waves do not reflect off of a single point of an object, nor do the sound
waves travel along a single line. Instead the sound waves emit within a range of
angles from the sensor. Thus multiple pulses from a stationary sonar range
finder will give different experimental values for $d$. Statistical analysis is
typically used to determine an estimate for object location. This analysis is
also used to filter out noise.

Sonar range finders do not come without their flaws. One of the largest
obstacles a range finder faces is object surface type and sonar noise. Every
object surface affects sound reflection differently. If an object's surface
tends to scatters sound waves then the receiver will not detect an echo (and
thus the robot will not know the object exists). Sonar noise can be created when
other range finders are present in the environment, or due to wave scattering.
Thus an ideal environment to use a range finder in is one in which few (if any)
other range finders are present, and the object surfaces are smooth (so as to
minimize scattering). 

\end{document}
